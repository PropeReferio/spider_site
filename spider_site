#!/usr/bin/env python

import argparse
from collections import deque
from urllib.parse import urlparse
import time
import timeit

import requests
from bs4 import BeautifulSoup
from tld import get_fld

def get_page(url, visited):
    """
    Gets the HTML of a page of the website
    :return: (soup or str, not sure yet) The HTML of the page
    """
    # Currently, this won't work with dynamically loaded websites.
    try:
        r = requests.get(url)
        if r.status_code != requests.codes.ok:
            print(f'There was a problem getting the page {url}')
    except Exception as e:
        print(e)
    else:
        soup = BeautifulSoup(r.content, "html.parser")
        return soup

def get_internal_links(soup, domain, unvisited_queue, visited):
    """
    Gets the internal links from the page. Filters out fragments. Checks
    for the use of the <base> tag, and makes adjustments as necessary.
    It also adds links with certain extensions directly to the visited list
    without attempting to parse them.
    :param html: The html of the page being parsed for internal links.
    :param domain: (str) The domain name of the site.
    """
    extensions = ['.jpg', '.jpeg', '.png', '.pdf']
    base_url = ''
    if soup.base:
        base_url = soup.base['href']
        if not base_url.endswith('/'):
            base_url += '/'
    all_links = soup.find_all('a')
    # TODO Add links to visited_raw_links, check if link in visited_raw_links before drop_params, is_internal, etc
    visited_raw_links = set()
    for i in range(len(all_links)):
        if all_links[i].has_attr('href'):
            cur_link = base_url + all_links[i]['href']
            cur_link = drop_params_queries_fragments(cur_link)
            if is_internal(cur_link, domain):
                if cur_link not in visited and cur_link not in unvisited_queue and not any([cur_link.endswith(ext) for ext in extensions]):
                    # TODO: Skip over links with extensions like .pdf, .jpg, .png, etc. Put them in visited, not unvisited.
                    unvisited_queue.append(cur_link)
                else:
                    visited.add(cur_link)
                    # This else block may be redundant. Consider removing it.

def drop_params_queries_fragments(url):
    """
    Takes a url and removes all params, queries, and fragments.
    :param url: The url to be cleaned
    :return: The cleaned url
    """
    link = urlparse(url)
    clean_link = link.scheme + '://' + link.netloc + link.path
    return clean_link

def is_internal(url, domain):
    """
    Returns true if URL is part of the domain AND there's no query
    to share the page, else False.
    :param url: (str) url of the link being checked
    :param domain: (str) url of the domain being checked
    :return: Boolean
    """
    parsed = urlparse(url)
    return parsed.netloc.endswith(domain)

def Main():
    """
    Checks all the links on a website, visiting each link at the same domain,
    adding unvisited links to a queue, and adding visited links to a set called
    visited. Use the visited set to avoid visiting those links again.
    Wordlists come from https://github.com/danielmiessler/SecLists.
    """
    start = time.time()
    parser = argparse.ArgumentParser(description="A script that tries to spider an entire website\
                                                 from a starting page provided by the user.")
    parser.add_argument('-f',
                        '--file',
                        help="File name to write output to.",
                        type=str)
    parser.add_argument('-u',
                        '--url',
                        help="URL starting point for finding all pages on the site.",
                        type=str,
                        required=True)
    parser.add_argument('-s',
                        '--subdomains',
                        help="Use this flag if you want to also spider to subdomains.",
                        action='store_true',
                        default=False)
    args = parser.parse_args()

    unvisited_queue = deque()
    # Searching through a set is much faster than through a list
    visited = set()

    # This should be its own function
    parsed = urlparse(args.url)
    url = f"http://{parsed.netloc}{parsed.path}"
    if args.subdomains:
        domain = get_fld(url, fix_protocol=True)
    else:
        domain = f"{parsed.netloc}"
    # line above turns 'www.google.co.uk' into 'google.co.uk'
    unvisited_queue.append(url)

    while len(unvisited_queue) > 0:
        cur_url = unvisited_queue.popleft()
        soup = get_page(cur_url, visited)
        visited.add(cur_url)
        if soup is not None:
            get_internal_links(soup, domain, unvisited_queue, visited)

    # TODO Determine if we should be dropping params and queries
    # TODO I need an option to also spider subdomains, default false.
    output = '\n'.join(sorted(list(visited)))
    print(f"\nHere's a list of links found on the site: \n\n{output}\n{round(time.time() - start, 2)} seconds elapsed")

    if args.file:
        with open(args.file, 'w') as file:
            file.write(output)

if __name__ == "__main__":
    Main()
